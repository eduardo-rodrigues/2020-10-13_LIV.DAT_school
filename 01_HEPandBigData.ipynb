{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h1><b><span style=\"color:blue\">Big Data Python ecosystem for HEP</span></b></h1>\n",
    "<h3>Eduardo Rodrigues<br>University of Liverpool</h3>\n",
    "\n",
    "<h3><span style=\"color:gray\">LIV.DAT, <a href=\"https://indico.ph.liv.ac.uk/event/103/\">STFC School on Data Intensive Science</a>, 12-16 October 2020</span></h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "Data analysis in High Energy Physics (HEP) has evolved considerably in recent years. In particular, the role of Python has been gaining\n",
    "much momentum, sharing at present the show with C++ as a language of choice.\n",
    "Several (community) domain-specific projects have seen the day, providing (HEP) data analysis packages that profit from, and talk to well with,\n",
    "the huge Python scientific ecosystem, which navigates around NumPy and friends.\n",
    "In this \"Big Data Python ecosystem for HEP\" session I will present and discuss a large set of this new HEP ecosystem ever more used by analysts\n",
    "across several experiments such as the LHC experiments but also Belle II, KM3NeT and others.\n",
    "Ample time will be provided to \"play around\" with the material, in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tutorial material\n",
    "\n",
    "- Notebooks GitHub repository https://github.com/eduardo-rodrigues/2020-10-13_LIV.DAT_school.\n",
    "- Can be nicely viewed [with nbviewer](https://nbviewer.jupyter.org/github/eduardo-rodrigues/2020-10-13_LIV.DAT_school/tree/master/).\n",
    "\n",
    "Please be patient - the repository takes 5-10 minutes to load completely on Binder ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Particle Physics and Big Data**\n",
    " \n",
    "A lot of what has happened in the HEP Python ecosystem can be thought of as trying to bridge the Particle Physics and Big Data worlds and profit from what the Data Science scientific software stack has to offer.\n",
    "\n",
    "We will come back to software, but what about the data itself? Is that \"Big Data\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CERN [ROOT team](https://root.cern/) advertises that of the order of 1 EB of data exists right now in the `.root` format.\n",
    "\n",
    "<center><img src=\"images/intro_ROOT.png\" width=\"70%\"/></center>\n",
    "\n",
    "Impressive. **We are already in the exascale era!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the record, the CERN Data Centre had accumulated more than 200 PB of data back in 2017 already!\n",
    "[[CERN news, July 6, 2017](https://home.cern/news/news/computing/cern-data-centre-passes-200-petabyte-milestone)]:<br>\n",
    "<center><img src=\"images/intro_CERN_200PB.png\" width=\"40%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And in just an extra 1.5 years, 50% more data got saved in CERN's Data Centre.\n",
    "Citing the [CERN news, March 1, 2019](https://home.cern/news/news/computing/lhc-pushing-computing-limits),<br>\n",
    "> \"*The CERN Advanced Storage system (CASTOR), which relies on a tape-based backend for permanent data archiving, reached 330 PB of data (equivalent to 330 million gigabytes) stored on tape, an equivalent of over 2000 years of 24/7 HD video recording. In November 2018 alone, a record-breaking 15.8 PB of data were recorded on tape, a remarkable achievement given that it corresponds to more than what was recorded during the first year of the LHC’s Run 1.*\"\n",
    "\n",
    "In fact, in 2018, over 115 PB of data in total (including about 88 PB of LHC data) were recorded on tape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The accummulation of data generated by the LHCb experiments alone, over a decade-ish, speaks for itself, as seen by this graph on CERN computing:<br> \n",
    "data (in terabytes) recorded on tapes at CERN month-by-month (2010–2018) [[ref](https://cds.cern.ch/images/CERN-HOMEWEB-PHO-2019-003-1)]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/intro_CERN_Graphique_DataTapes_web_1920x1080p.png\" width=\"75%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**And what about the outside world?**\n",
    "\n",
    "Let's look at Amazon for the sake of argument [[ref](https://aws.amazon.com/snowmobile/)]:\n",
    ">\"*AWS Snowmobile is an Exabyte-scale data transfer service used to move extremely large amounts of data to AWS. You can transfer up to 100PB per Snowmobile, a 45-foot long ruggedized shipping container, pulled by a semi-trailer truck.*\"\n",
    "<center><img src=\"images/intro_aws-snowmobile.jpg\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> \"Each Snowmobile includes a network cable connected to a high-speed switch capable of supporting 1Tbps of data transfer spread across multiple 40Gbps connections.\" \n",
    "\n",
    "This is to be compared with the throughput of 1-2 Tbps the LHCb experiment's first high-level triigger HLT1 (partial reconstruction on GPUs) will put to buffer while the real-time calibration and alignment is run, which is needed to digest the data in the HLT2 (full reconstruction)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These solutions are necessary to massage the shear amounts of data being produced, and going to be produced, worldwide [[ref](https://databricks.com/p/ebook/a-guide-to-data-analytics-and-ai-at-scale)]:\n",
    "<center><img src=\"images/intro_global_data_growth.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Is it relevant and useful to learn non-HEP tools?**\n",
    "\n",
    "We've just quickly recalled that data requirements for Particle Physics match those of the Big Data world. Huge amounts of data are in fact used by companies worldwide for just about any business, see for example the report \"State of Data Science and Analytics, IDC InfoBrief, April 2019\":\n",
    "<center><img src=\"images/intro_data_in_business.png\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "International surveys indicate ***over 50M data workers worldwide***! Can we really compete in terms of tools for data analysis?\n",
    "... Or should we rather try and profit from, and even contribute to, the huge ecosystem available to do Data Science?\n",
    "\n",
    "Anyway, what are data scientists, data engineers, etc., using for their daily work? That largely involves to some (larger and larger) extent Machine Learning, statistics, and even AI. International surveys give a hint ... the tools are dominated by Python tools ...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the 2019 Figure Eight report \"The State of AI and Machine Learning\",\n",
    "> \"Some popular frameworks and tools technical practitioners prefer in different stages of the ML pipeline are: Numpy and Pandas for loading data; Matplotlib for visualization; Scikit-learn and TensorFlow (including Keras) for ML models.\":<br>\n",
    "<center><img src=\"images/intro_popular_python_packages.png\" width=\"40%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **The reign of Python**\n",
    "\n",
    "One may say that 2019 was the year for Python. Now it simply is more popular than anything else:\n",
    "\n",
    "<center><img src=\"images/intro_python_popularity.png\" width=\"55%\"/></center>\n",
    "\n",
    "The trend is only more clear when checking the popularity of programming languages for Machine Learning, BTW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle Physics is clearly following this same trend. Here is an example study from CMS:\n",
    "\n",
    "<center><img src=\"images/intro_python_adoption_CMS.png\" width=\"55%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Take aways**\n",
    "\n",
    "- Particle Physics and Data Science deal with Big Data and share requirements.\n",
    "- The Data Science world has over the years built an extensive, powerful, well maintained and documented software ecosystem.\n",
    "- It would be real shame for Particle Physics not to profit from it, as user but potentially also as a contributor.\n",
    "- Python is the programming language of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to explore these lectures**\n",
    "\n",
    "- Below is a list of available notebooks. They are self-consistent, for you to run though them at your own pace and leisure. Run what topics sound appealing to you ...\n",
    "\n",
    "- You will also find some corresponding `XX_exercises.ipynb`. As the names indicate, they suggest exercises beyond the introduction to the various topics and related packages. Again, feel free to decide what to try out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Notebooks\n",
    "\n",
    "- **`01_HEPandBigData.ipynb`**:<br>\n",
    "this introductive notebook ;-).\n",
    "- **`02_PyHEP.ipynb`**:<br>\n",
    "presentation of the \"Python in HEP\" community and the HEP Big Data ecosystem.\n",
    "- **`0X_Scikit-HEP_YYY.ipynb`**:<br>\n",
    "notebooks with Scikit-HEP project packages for a series of analysis topics.\n",
    "- **`0X_exercises.ipynb`**:<br>\n",
    "suggested exercises related to the topics presented in notebooks `0X_`.\n",
    "- **`10_exploration.ipynb`**:<br>\n",
    "personal exploration of a common fitting problem in HEP making use of the model manipulation and fitting library based on TensorFlow `zfit`."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
